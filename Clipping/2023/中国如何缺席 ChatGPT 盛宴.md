---
created: 2023-02-10T21:27:35 (UTC +08:00)
tags: []
source: https://archive.is/6eeyj
author: 红博士说
---

# 中国如何缺席ChatGPT盛宴

> archived 9 Feb 2023 09:00:33 UTC

目录：  

1.  ChatGPT 编年史
    
2.  我们如何错过 GPT 盛宴
    
3.  GPT 大语言模型能实现 AGI 吗
    
4.  连载话题预告
    

### ChatGPT 编年史

我们来梳理一个时间轴。ChatGPT 是对话式 UI + GPT–3.5 系列模型，我们以最具代表性的论文、模型、API 为主线，梳理到今天。

2020 之前

- 2017 年 6 月，Google 发布 Transformer 论文。

- 2017 年 6 月，7 月，OpenAI 发布人类喜好的强化学习算法、PPO 算法，都是 ChatGPT 用到的算法。

- 2018 年 6 月，OpenAI 发布 GPT-1.

- 2018 年 11 月，Google 发布 BERT，此后 NLP 领域主要基于这个框架研究下游任务。

- 2019 年 2 月，OpenAI 发布 GPT-2，OpenAI 获得了自信，此后专注于 GPT.

2020 年

- 年初，Covid-19 爆发。**中国闭关**。

- 1 月，OpenAI 发布语言模型的 Scaling Law（概念：模型能力跟参数规模、数据规模强相关），OpenAI 获得了在数据和参数规模上 Scaling-up 的信心。

- 5 月，GPT-3 论文发布。

- 6 月，**GPT-3 API 发布**。

- 9 月，ChatGPT 的关键原型算法相关论文发布。

- 12 月，欧洲机构发布用于 GPT-3 复现的开源数据集。

2021 年

- 7 月，OpenAI 发布 Copilot 原型算法。

- 8 月，Codex API 发布。

- 11 月，**GPT-3 API Public Release，不对中国开放**。

- **中国闭关**。

2022 年

- 1 月，GPT-3.5 API (text-davinci-002) 发布，该模型经过 Github 代码的训练加持，推理能力显著提升（该假设的因果关系待学术界论证），经过 Alignment 技术的加持，Follow 人类指令的能力显著提升，输出结果有用性和无害性显著提升。

- 3 月，GPT-3.5 论文发布，公开 Alignment 算法。

- 5 月，OpenAI Codex 已经被 70 个应用使用，包括微软收购的 Github 的 Copilot.

- 8 月，Stability AI 开源 StableDiffusion，文生图的算法的效果可用、速度可行、代码开源同时发生，引爆图片生成。一时间，在中国，AIGC 似乎就是图片生成的代名词。

- 9 月，Sequoia Capital 发布 Generative AI: A Creative New World 博客。

- 中国研究人员和开发者，没有 OpenAI 的 API 权限。但图片生成却人人都可以尝试，于是互联网似乎只注意到了图片生成，对 GPT 大语言模型的关注度进一步下降。

- 经过接近一年的 API 接入和 UI 探索、近一年的思维链（Chain of Thought）等 Prompt Engineering 技术试错、模型加速等技术（如 Flash Attention、Fixed-Point）带来的成本和延迟下降，GPT-3.5 的模型潜力得到开发（变得 Better、Faster and Cheaper）, Copy.ai, Jasper 等文本生成类公司的产品逐渐成熟。

- 11 月，OpenAI 发布 GPT3.5 API 的新模型 (text-davinci-003).

- **12 月 1 日，ChatGPT 发布**。Musk 等名流开始谈论 ChatGPT，引爆英文互联网。

- 12 月初，中国互联网的自媒体逐渐开始讨论 ChatGPT，主要以翻译 twitter 的方式。知乎上有学者开始反思。一周后，关注指数下降，两个月来只剩下 AI 自媒体把 ChatGPT 作为自己的主要关注内容。

- **中国闭关**。

2023 年

- 1 月，微软宣布投资 OpenAI 数十亿美元，并将 GPT 加入全家桶。

- 2 月，中国春节结束，微软和 Google 你方唱罢我登场，纳斯达克财报季，AI 被反复提起。中国互联网是认识微软的，ChatGPT 引爆中国互联网，关注指数飙升。

- **中国开放**。

值得注意的是，中国因为疫情闭关的三年，正是 OpenAI 的 GPT 发展、壮大、产品化的三年。

### 我们如何错过 GPT 盛宴？

历史回顾完了，那么为什么我们（中国，尤其是 AI 社区）没有更早地意识到，OpenAI 技术在应用层面的突破性？

意识到问题需要同时具备哪些条件：

1. 能够看且懂 OpenAI、DeepMind、Google 等机构的论文（代表人群：研究员）

2. 能够使用 OpenAI 的 API 探索论文里的模型 （代表人群：研究员里的尝鲜者）

3. 对硅谷的敏感性，经常看大家在用 OpenAI 的 API 做什么产品 （代表人群：VC）

这三类人在中国，我们粗估一下，第一类，大概有 1/100,000，第二类大概是第一类里的 1/1,000，第三类大概是 1/1,000,000. 三个条件，缺少一个，都无法意识到 OpenAI 发展到哪一步了。有哪个团队汇集了这三种人，并且他们有充分的碰撞？有哪个人是具备了这三种属性？ 雪上加霜的是，研究人员三年来被封在国内，没有出国参加过学术会议交流，甚至我猜很多人连线上会议都没有参加，很多东西我们从论文上是看不到的。

我们继续深挖。第一类人群中，又分成 NLP（自然语言处理）研究人员，其他 AI 研究人员（比如计算机视觉、语音识别、机器学习）。

中国 NLP 的研究群体里，基本上是把语言模型（尤其是 BERT，而不是 GPT）拿去应用在 NLP 的各种下游任务上，在学术界就是刷榜发论文，在工业界，就是拿去做客服机器人、写稿机器人、角色扮演机器人，研究方法也完全不同于 GPT 精髓——Scaling-up 和 Alignment。（几乎）没有人是把大语言模型（LLM）当做通用人工智能（AGI）的一种可能性来研究的。

其他 AI 研究人员，比如计算机视觉，大部分人还是专注在图像上，即使是用 Transformer，也是解决图像的问题，比如用 Transformer 来做自动驾驶、图像生成等。即使是 Tesla AutoPilot 的 AI 主管 Karpathy。Karpathy 在 2022 年上半年从 Tesla 裸辞，以独立研究员的身份，投身于大语言模型。

Karpathy 曾经说他过去十年痴迷于 AI 中取得最快进展的方向，并且曾经对语言模型非常感兴趣，但是却忽视了 scaling up 的力量，那就是简单的 Objective（next word）+ 简单的结构（Transformer）+ 足够的参数 + 足够的数据 (web text)，一个语言模型可以涌现出在小规模状态下看不到的能力，他曾像其他人一样（他应该指早期的 OpenAI），一度以为强化学习是 AGI 的路径，到头来却发现大语言模型是看起来最有希望的路径。在此之前，语言模型的研究人员，把精力过多地放在了具体任务上。

再说 AI 领域的另一个重要群体——计算机视觉（Computer Vision) 群体。在 2012 年开始的深度学习浪潮里，计算机视觉一直是应用最广、商业化最成功的方向，吸引了太多 AI 研究员的精力，从图像分类、检测、分割到识别，从图像到视频，从高层视觉到底层视觉，我们在卷积神经网络上卷出了一个又一个新高度。一个 YOLO 目标检测框架，被迭代到原作者都放弃了，还有人给推到了 v7 版本。最具代表性的是计算机视觉的登月工程——自动驾驶，它需要成像、识别、合成、建图、规划等几乎所有的视觉 AI 技术加持，从 CNN 时代到 Transformer 时代，不断地拉更多的人下水，但直到今天，全自动驾驶的方案仍未收敛。马斯克定义的问题是对的，自动驾驶是一个 real-world AI 问题，但显然特斯拉的方案并没有为全自动驾驶准备好。

NLP 圈的小家碧玉，CV 圈的隔行隔山，疫情闭关三年，互联网信息不通。这些因素叠加起来，整个中文世界，形成了一个信息茧房。10 年来，我们以为自己积攒的 AI 算法、数据、应用的优势，如今变成中美巨大的鸿沟。这个时候，我们甚至没有一个新闻调查，把这件事的来龙去脉，挖它个底朝天。

另一个问题是，我们的中文互联网不足以提供高质量的训练数据。什么是高质量的数据？比如维基百科、高质量的活跃论坛、专业新闻、学术论文、高质量代码、图书。

我们看看 GPT–3 的训练数据是什么。权重最大的数据集是 OpenWebText（开源版本）, 数据是从 Reddit 论坛上收集的 URL，再把内容抓取下来。Common Crawl 是一个开放的互联网数据存档（英文占一半，中文大概 5%）。其他一些代表性的数据包括 Wikipedia 维基百科，Books 开放图书，Stack Exchange 技术问答社区，Github 代码，ArXiv 论文，RealNew 新闻存档，PubMed 医疗数据。可以看到，由中文互联网产生的数据，比例低到可以忽略。这也是困扰很多试图训练中文大模型的问题，但实际上，ChatGPT 的用中文沟通的能力，已经远超那些专门的中文大语言模型了，背后原因是 GPT 隐式学到的翻译能力。

没有好的中文数据，我们就只能搭全球互联网的数据顺风车。上面这些优质数据的产生，需要开放的社区，我们似乎无解。

### GPT 大语言模型能实现 AGI 吗？

基于 GPT 的 LLM，仅仅依赖语言，大概率无法实现 AGI，而只是” 通往 AGI 的高速公路的一个出口（Yann Lecun）“。但 LLM 足以把互联网基础设施搞个天翻地覆，它同时具备了 Logic 和 Memory。Logic 是推理能力，Memory 是对高频知识的记忆，显然 Memory 可以分为片上和片外，片上有限，片外无限。下一步，我们只需要专注于把 LLM 的 Logic 推到极致，把大部分低频 Memory offload 到模型以外，配以搜索等查询技术，就可以实现对整个互联网前后端的重构。我们远远没有吃尽 scaling-law 的红利，限制我们的，只有集成电路的摩尔定律和制造能力、能源的价格、数据的获取。

集成电路方面，以 Chiplet 为代表的系统摩尔定律还不够，人们需要能够 scaling-up 的 Foundry。

能源方面，太阳能和风能 + 能源存储能够解决很多问题，更加激动人心的是以 Helion 为代表的核聚变技术，则有机会把能源价格降低一个量级，然后更多。

数据方面，目前的 GPT 模型依赖互联网文本数据，这会用尽，没关系，现实世界的数据是无限的。

### 连载话题预告

今天先写到这儿。

计划中：

- OpenAI 的故事

- AI Alignment

- AI 与资本主义

- AI 与教育

- AGI 时代的人

By 红博士, 2023 年 2 月 8 日
